{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 436 CS5310 - Computer Vision - Assignment 4\n",
    "\n",
    "*__Submission Instructions:__*\n",
    "- Rename this notebook to `PA4_rollnumber.ipynb` before submission on LMS.\n",
    "- Code for all the tasks must be written in this notebook (you do not need to submit any other files).\n",
    "- The output of all cells must be present in the version of the notebook you submit.\n",
    "- The university honor code should be maintained. Any violation, if found, will result in disciplinary action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.models import Model\n",
    "from keras.applications import vgg16\n",
    "from glob import glob\n",
    "import cv2\n",
    "import json\n",
    "import pickle\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this assignment you will be exploring a few important concepts used in the deep learning projects:\n",
    "- Training image classification algorithms using Deep Learning\n",
    "- Dataset Analyses \n",
    "- Testing deep learning classifier with the test data\n",
    "- Fine-tuning / Transfer Learning\n",
    "\n",
    "We will be using a customized datasets, the links to download the data are provided to you. You will also be working with pretrained models, which could be downloaded from keras applications. You are **highly** encouraged to explore the images in dataset and model architectures in order to get the most out of this assignment. \n",
    "\n",
    "**_Dataset:_**\n",
    "- D1- Test Data for evaluating the pretrained model (VGG-16) can be found in the folder \"test-multiple_fruits\" [here](https://drive.google.com/drive/folders/1ViePNUqS3LmPkaW6vJ1cRb4YGyIkWpqz?usp=sharing)\n",
    "- D2- Data to be used for fine-tuning VGG-16 for 75 classes of fruits could be downloaded from the same link in the folder \"fruits-trainValidate\"\n",
    "\n",
    "\n",
    "**_Pretrained Models:_** \n",
    "Can be found [here](https://keras.io/applications/#applications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Preparation\n",
    "\n",
    "Evaluate the performance of a pretrained network (VGG-16) for the test-multiple_fruits data downloaded by predicting labels of each image. You will:\n",
    "- Download the VGG16 model and compile it with pretrained weights from imagenet.\n",
    "- Obtain predictions for the test-multiple_fruits D1 dataset\n",
    "- Print the predictions for all the test image from D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 1172s 2us/step\n"
     ]
    }
   ],
   "source": [
    "vgg16_pretrained =keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "vgg16_pretrained.save('./vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umair/anaconda3/envs/DL/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "def pretrained_test(model_path,test_dir):\n",
    "    pass\n",
    "vgg16_pretrained = load_model('./vgg16')\n",
    "vgg16_pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "vgg16_pretrained.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "test_dir = './test-multiple_fruits/'\n",
    "test_img_paths = glob(test_dir + '*')\n",
    "# print(len(test_img_paths))\n",
    "allimages = np.array([ cv2.resize(cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB),(224, 224)) for path in test_img_paths])\n",
    "print(allimages.shape)\n",
    "#     vgg16_pretrained.compile()\n",
    "#     vgg16.predict()\n",
    "    \n",
    "\n",
    "# Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = vgg16_pretrained.predict(allimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107 509 721 591 591 721 679 679 584 679 562 735 971 723 679 938 679 107\n",
      " 710 679 679 599 417 868 828 956 522 868 868 868 952 584 748 794 955 987\n",
      " 928 973 115   6 973 973 973 463 722]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([np.argmax(pred) for pred in predictions])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cocos_kiwi_orange_dates_salak_plum_tamarilo_maracuja.jpg',\n",
       " 'apple.jpg',\n",
       " 'apple_apricot_nectarine_peach_peach(flat)_pomegranate_pear_plum.jpg',\n",
       " 'apple_apricot_peach_peach(flat)_pomegranate_pear_plum.jpg',\n",
       " 'apple_apricot_peach_peach(flat)_pomegranate_pear_plum_2.jpg',\n",
       " 'apple_apricot_peach_peach(flat)_pomegranate_pear_plum_3.jpg',\n",
       " 'apple_grape.jpg',\n",
       " 'apple_grape2.jpg',\n",
       " 'apple_pear.jpg',\n",
       " 'cherry(rainier).jpg',\n",
       " 'cherry.jpg',\n",
       " 'cherry2.jpg',\n",
       " 'cherry3.jpg',\n",
       " 'cherry_strawberries.jpg',\n",
       " 'cherry_strawberries2.jpg',\n",
       " 'cocos.jpg',\n",
       " 'cocos_kiwi_orange_dates_salak_plum_tamarilo_maracuja2.jpg',\n",
       " 'cocos_kiwi_orange_dates_salak_plum_tamarilo_maracuja3.jpg',\n",
       " 'dates.jpg',\n",
       " 'grape_pear_mandarine.jpg',\n",
       " 'grape_pear_mandarine2.jpg',\n",
       " 'huckleberry.jpg',\n",
       " 'kachi.jpg',\n",
       " 'kiwi.jpg',\n",
       " 'limes.jpg',\n",
       " 'lychee.jpg',\n",
       " 'mango.jpg',\n",
       " 'pear.jpg',\n",
       " 'physalis.jpg',\n",
       " 'physalis2.jpg',\n",
       " 'pomegranate.jpg',\n",
       " 'pomegranate2.jpg',\n",
       " 'raspberry.jpg',\n",
       " 'raspberry2.jpg',\n",
       " 'raspberry3.jpg',\n",
       " 'raspberry4.jpg',\n",
       " 'raspberry5.jpg',\n",
       " 'raspberry_apple.jpg',\n",
       " 'salak.jpg',\n",
       " 'salak2.jpg',\n",
       " 'strawberries.jpg',\n",
       " 'strawberries2.jpg',\n",
       " 'strawberries3.jpg',\n",
       " 'strawberries4.jpg',\n",
       " 'tangelo.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = glob('./test-multiple_fruits/'+'*')\n",
    "images = [path.replace('./test-multiple_fruits/','') for path in images]\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-94b34de6f4aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "labels_dict = {}\n",
    "with open('./labels.pkl','rb') as f:\n",
    "    labels_dict = pickle.load(f)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for index,pred in enumerate(y_pred):\n",
    "    key = images[index]\n",
    "    results[key] = labels_dict[pred]\n",
    "    print(f\"{key} ==> {results[key]}\")\n",
    "\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator for Task 2\n",
    "You could either use this batch image generator or could write your own batch generator if required for fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37836 images belonging to 75 classes.\n",
      "Found 12709 images belonging to 75 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image Augmentation\n",
    "#Replace the input training/validation directories with the path to your training and validation splits\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "INPUT_TRAINING_DIRECTORY = './fruits-trainValidate/fruits-360/Training/'\n",
    "INPUT_VALIDATION_DIRECTORY = './fruits-trainValidate/fruits-360/Validation/'\n",
    "IMAGE_SIZE = (100,100)\n",
    "batch_size = 64\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "                                    rescale=1./255,   # all pixel values will be between 0 an 1\n",
    "                                    shear_range=0.2, \n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    preprocessing_function=preprocess_input)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "\n",
    "training_generator = training_datagen.flow_from_directory(INPUT_TRAINING_DIRECTORY, target_size = IMAGE_SIZE, batch_size = batch_size, class_mode = 'categorical')\n",
    "validation_generator = validation_datagen.flow_from_directory(INPUT_VALIDATION_DIRECTORY, target_size = IMAGE_SIZE, batch_size = batch_size, class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# x,y = next(training_generator)\n",
    "# plt.imshow(x[-3])\n",
    "# print(np.argmax(y[-3]))\n",
    "\n",
    "training_labels = glob(INPUT_TRAINING_DIRECTORY + \"*\")\n",
    "training_labels = [label.split('/')[-1] for label in training_labels]\n",
    "print(len(training_labels))\n",
    "validation_labels = glob(INPUT_VALIDATION_DIRECTORY + \"*\")\n",
    "validation_labels = [label.split('/')[-1] for label in validation_labels]\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Transfer Learning\n",
    "\n",
    "Next you will employ Transfer Learning and finetune the pretrained vgg-16 model you used in Task1 to better fit the fine-tune dataset D2 for 75 classes of fruits (details available in the readme file of dataset folder). You will:\n",
    "\n",
    "- Change the number of nodes in the last FC layer according to the number of classes i.e. 5 \n",
    "- Freeze everything except the FC layers and train it using the train split of D2 (using appropriate hyperparameters), validating the network for validation split of data.\n",
    "- Train (Finetune) the dataset with training split and validate it using validation split\n",
    "- Plot loss/accuracy vs epochs curves for your simulation\n",
    "\n",
    "*You can use scikit-learn's `metrics.confusion_matrix` function. Consult the relevant documentation.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#def finetune_test(...):\n",
    "#    pass\n",
    "\n",
    "# Hints\n",
    "num_classes = 75\n",
    "# vgg_imagenet = load_model('./vgg16')\n",
    "vgg_imagenet = vgg16.VGG16(include_top=False, weights='imagenet',input_shape=(100,100,3))\n",
    "adam = Adam(0.0001)\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# vgg_imagenet.layers.pop()\n",
    "vgg_imagenet.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "vgg_imagenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                345675    \n",
      "=================================================================\n",
      "Total params: 15,060,363\n",
      "Trainable params: 15,060,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for lyr in vgg_imagenet.layers:\n",
    "    lyr.freeze = True\n",
    "# add new FC layers here\n",
    "flatten = Flatten()(vgg_imagenet.layers[-1].output)\n",
    "new_pred = Dense(num_classes,activation='softmax')(flatten)\n",
    "vgg_new = Model(vgg_imagenet.layers[0].input,new_pred)\n",
    "adam = Adam(0.001)\n",
    "# for lyr in vgg_new.layers:\n",
    "#     lyr.trainable = False\n",
    "# vgg_new.layers[-1].trainable = True\n",
    "vgg_new.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "vgg_new.summary()\n",
    "# print summary and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('./models/' +'vgg_new'+'-{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                             monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "callbacks = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  1/591 [..............................] - ETA: 5:00:14 - loss: 4.8077"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-54f98b3e7285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m hist = vgg_new.fit_generator(training_generator, epochs=1, steps_per_epoch=37836//batch_size, \n\u001b[1;32m      2\u001b[0m                            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12709\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                            callbacks=callbacks, verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = vgg_new.fit_generator(training_generator, epochs=1, steps_per_epoch=37836//batch_size, \n",
    "                           validation_data=validation_generator, validation_steps=12709//batch_size, \n",
    "                           callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss accuracy curves using matplot lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Network Evaluation\n",
    "\n",
    "Next you will test your finetuned model by plotting a confusion matrix between classes predicted. You will:\n",
    "\n",
    "- Load the saved finetuned network\n",
    "- Test your model for images in validation folder of D2\n",
    "- Construct a multiclass confusion matrix (for any 10 classes) for actual and predicted class of each image and visualize the confmatrix as a heatmap\n",
    "\n",
    "*You can use scikit-learn's `metrics.confusion_matrix` function. Consult the relevant documentation.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From shaypal5's github\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_true = []\n",
    "\n",
    "steps = 12709//batch_size\n",
    "s = 0\n",
    "for x,y in validation_generator:\n",
    "    if s < steps:\n",
    "        y_pred = vgg_new.predict(x,batch_size)\n",
    "        all_predictions.extend(y_pred)\n",
    "        all_true.extend(y)\n",
    "    else:\n",
    "        break\n",
    "    s+=1\n",
    "        \n",
    "all_predictions = np.array(all_predictions)\n",
    "all_true = np.array(all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2890d9fdd511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconf_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print_confusion_matrix(conf_mat,validation_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(all_true.argmax(axis=1),all_predictions.argmax(axis=1))\n",
    "\n",
    "# print_confusion_matrix(conf_mat,validation_labels)\n",
    "print_confusion_matrix(conf_mat,[i for i in range(75)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Prediction\n",
    "\n",
    "Next you will test your finetuned model for test-multiple_fruits images and compare the result with the results of Task 1. You will:\n",
    "\n",
    "- Predict labels for images in test-multiple_fruits D1 folder using finetuned network\n",
    "- Compare your result qualitativly and quatitatively (by visualizing some of the comparing images with their respective labels)\n",
    "- Analyse and discuss the improvement of results (if found any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = vgg_new.predict(allimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_pred = np.array([np.argmax(pred) for pred in new_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocos_kiwi_orange_dates_salak_plum_tamarilo_maracuja.jpg ==> Pear Abate\n",
      "apple.jpg ==> Pear Abate\n",
      "apple_apricot_nectarine_peach_peach(flat)_pomegranate_pear_plum.jpg ==> Pear Abate\n",
      "apple_apricot_peach_peach(flat)_pomegranate_pear_plum.jpg ==> Pear Abate\n",
      "apple_apricot_peach_peach(flat)_pomegranate_pear_plum_2.jpg ==> Pear Abate\n",
      "apple_apricot_peach_peach(flat)_pomegranate_pear_plum_3.jpg ==> Pear Abate\n",
      "apple_grape.jpg ==> Pear Abate\n",
      "apple_grape2.jpg ==> Pear Abate\n",
      "apple_pear.jpg ==> Pear Abate\n",
      "cherry(rainier).jpg ==> Pear Abate\n",
      "cherry.jpg ==> Pear Abate\n",
      "cherry2.jpg ==> Pear Abate\n",
      "cherry3.jpg ==> Pear Abate\n",
      "cherry_strawberries.jpg ==> Pear Abate\n",
      "cherry_strawberries2.jpg ==> Pear Abate\n",
      "cocos.jpg ==> Pear Abate\n",
      "cocos_kiwi_orange_dates_salak_plum_tamarilo_maracuja2.jpg ==> Pear Abate\n",
      "cocos_kiwi_orange_dates_salak_plum_tamarilo_maracuja3.jpg ==> Pear Abate\n",
      "dates.jpg ==> Pear Abate\n",
      "grape_pear_mandarine.jpg ==> Pear Abate\n",
      "grape_pear_mandarine2.jpg ==> Pear Abate\n",
      "huckleberry.jpg ==> Pear Abate\n",
      "kachi.jpg ==> Pear Abate\n",
      "kiwi.jpg ==> Pear Abate\n",
      "limes.jpg ==> Pear Abate\n",
      "lychee.jpg ==> Pear Abate\n",
      "mango.jpg ==> Pear Abate\n",
      "pear.jpg ==> Pear Abate\n",
      "physalis.jpg ==> Pear Abate\n",
      "physalis2.jpg ==> Pear Abate\n",
      "pomegranate.jpg ==> Pear Abate\n",
      "pomegranate2.jpg ==> Pear Abate\n",
      "raspberry.jpg ==> Pear Abate\n",
      "raspberry2.jpg ==> Pear Abate\n",
      "raspberry3.jpg ==> Pear Abate\n",
      "raspberry4.jpg ==> Pear Abate\n",
      "raspberry5.jpg ==> Pear Abate\n",
      "raspberry_apple.jpg ==> Pear Abate\n",
      "salak.jpg ==> Pear Abate\n",
      "salak2.jpg ==> Pear Abate\n",
      "strawberries.jpg ==> Pear Abate\n",
      "strawberries2.jpg ==> Pear Abate\n",
      "strawberries3.jpg ==> Pear Abate\n",
      "strawberries4.jpg ==> Pear Abate\n",
      "tangelo.jpg ==> Pear Abate\n"
     ]
    }
   ],
   "source": [
    "for index,pred in enumerate(new_y_pred):\n",
    "    key = images[index]\n",
    "    results[key] = training_labels[pred]\n",
    "    print(f\"{key} ==> {results[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: Pear Abate\n",
      "Input Image\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Image data cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-5523c940ff1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predicted label: {training_labels[new_y_pred[0]]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input Image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2701\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2702\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    640\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[1;32m    641\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[0;32m--> 642\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image data cannot be converted to float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         if not (self._A.ndim == 2\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Predicted label: {training_labels[new_y_pred[0]]}\")\n",
    "print(\"Input Image\")\n",
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
