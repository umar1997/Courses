{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PA4_DeepLearning.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"SQrZOYYT6GHw","colab_type":"text"},"cell_type":"markdown","source":["# CS 436 CS5310 - Computer Vision - Assignment 4\n","\n","*__Submission Instructions:__*\n","- Rename this notebook to `PA4_rollnumber.ipynb` before submission on LMS.\n","- Code for all the tasks must be written in this notebook (you do not need to submit any other files).\n","- The output of all cells must be present in the version of the notebook you submit.\n","- The university honor code should be maintained. Any violation, if found, will result in disciplinary action. "]},{"metadata":{"id":"wHbV3Dyy6GHz","colab_type":"code","colab":{}},"cell_type":"code","source":["#Import all the required libraries\n","\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn')\n","import seaborn as sns\n","import glob\n","from keras.models import load_model\n","from keras.applications import vgg16\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from keras.applications.vgg16 import decode_predictions\n","from keras.applications.vgg16 import VGG16"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tuJKP5KY6GIE","colab_type":"code","outputId":"dc0ca07e-c48f-432c-f4db-f30fb51e837f","colab":{"base_uri":"https://localhost:8080/","height":138},"executionInfo":{"status":"ok","timestamp":1555955677878,"user_tz":-300,"elapsed":81295,"user":{"displayName":"Umar Salman","photoUrl":"","userId":"02559536710924243729"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","!ls \"/content/gdrive/My Drive/Colab Notebooks\"\n","!unzip -qq \"/content/gdrive/My Drive/Colab Notebooks/test-multiple_fruits.zip\"\n","!unzip -qq \"/content/gdrive/My Drive/Colab Notebooks/fruits-trainValidate.zip\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","fruits-trainValidate.zip  PA4_DeepLearning.ipynb  test-multiple_fruits.zip\n"],"name":"stdout"}]},{"metadata":{"toc-hr-collapsed":false,"id":"sv1tJfVz6GIL","colab_type":"text"},"cell_type":"markdown","source":["## Overview\n","\n","In this assignment you will be exploring a few important concepts used in the deep learning projects:\n","- Training image classification algorithms using Deep Learning\n","- Dataset Analyses \n","- Testing deep learning classifier with the test data\n","- Fine-tuning / Transfer Learning\n","\n","We will be using a customized datasets, the links to download the data are provided to you. You will also be working with pretrained models, which could be downloaded from keras applications. You are **highly** encouraged to explore the images in dataset and model architectures in order to get the most out of this assignment. \n","\n","**_Dataset:_**\n","- D1- Test Data for evaluating the pretrained model (VGG-16) can be found in the folder \"test-multiple_fruits\" [here](https://drive.google.com/drive/folders/1ViePNUqS3LmPkaW6vJ1cRb4YGyIkWpqz?usp=sharing)\n","- D2- Data to be used for fine-tuning VGG-16 for 75 classes of fruits could be downloaded from the same link in the folder \"fruits-trainValidate\"\n","\n","\n","**_Pretrained Models:_** \n","Can be found [here](https://keras.io/applications/#applications)\n"]},{"metadata":{"id":"wLC6X1QY6GIO","colab_type":"text"},"cell_type":"markdown","source":["## Task 1: Data Preparation\n","\n","Evaluate the performance of a pretrained network (VGG-16) for the test-multiple_fruits data downloaded by predicting labels of each image. You will:\n","- Download the VGG16 model and compile it with pretrained weights from imagenet.\n","- Obtain predictions for the test-multiple_fruits D1 dataset\n","- Print the predictions for all the test image from D1"]},{"metadata":{"id":"gLrtYD7y6GIQ","colab_type":"code","outputId":"1bf16e73-e60e-4706-fb08-8f696f3a42ea","executionInfo":{"status":"ok","timestamp":1555955826878,"user_tz":-300,"elapsed":41350,"user":{"displayName":"Umar Salman","photoUrl":"","userId":"02559536710924243729"}},"colab":{"base_uri":"https://localhost:8080/","height":773}},"cell_type":"code","source":["def pre_trained_test():\n","  model = VGG16()\n","  for fileName in glob.glob('test-multiple_fruits/*.jpg'):\n","    image = load_img(fileName, target_size=(224, 224))\n","    image = img_to_array(image)\n","    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","    image = preprocess_input(image)\n","    yhat = model.predict(image)\n","    label = decode_predictions(yhat)\n","    label = label[0][0]\n","    print('%s (%.2f%%)' % (label[1], label[2]*100))\n","  \n","pre_trained_test()\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["corn (31.64%)\n","strawberry (25.86%)\n","custard_apple (84.63%)\n","rotisserie (62.49%)\n","lemon (28.92%)\n","barrow (11.06%)\n","hermit_crab (20.62%)\n","cucumber (15.12%)\n","buckeye (61.51%)\n","abacus (38.20%)\n","guacamole (62.88%)\n","hip (67.51%)\n","pomegranate (17.86%)\n","strawberry (61.05%)\n","buckeye (6.36%)\n","Granny_Smith (68.36%)\n","orange (87.63%)\n","tray (38.76%)\n","Granny_Smith (70.41%)\n","orange (59.01%)\n","pot (29.19%)\n","fig (28.74%)\n","hip (32.14%)\n","pomegranate (35.17%)\n","pineapple (22.97%)\n","pomegranate (65.32%)\n","hotdog (49.03%)\n","pomegranate (24.42%)\n","strawberry (30.33%)\n","mixing_bowl (31.72%)\n","fig (82.88%)\n","strawberry (93.30%)\n","strawberry (41.33%)\n","hip (23.56%)\n","Granny_Smith (97.14%)\n","teddy (13.77%)\n","bucket (21.57%)\n","strawberry (74.01%)\n","grocery_store (29.84%)\n","daisy (18.28%)\n","greenhouse (29.44%)\n","butternut_squash (10.44%)\n","pot (47.05%)\n","tray (28.33%)\n","honeycomb (53.67%)\n"],"name":"stdout"}]},{"metadata":{"id":"K-f48kkI6GIX","colab_type":"text"},"cell_type":"markdown","source":["## Batch Generator for Task 2\n","You could either use this batch image generator or could write your own batch generator if required for fine-tunning"]},{"metadata":{"id":"WOGVuiRI6GIY","colab_type":"code","outputId":"b71264f2-c51b-4188-c723-c5c55431a84e","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1555955837302,"user_tz":-300,"elapsed":3449,"user":{"displayName":"Umar Salman","photoUrl":"","userId":"02559536710924243729"}}},"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.vgg16 import preprocess_input\n","\n","training_datagen = ImageDataGenerator(\n","                                    rescale=1./255,   # all pixel values will be between 0 an 1\n","                                    shear_range=0.2, \n","                                    zoom_range=0.2,\n","                                    horizontal_flip=True,\n","                                    preprocessing_function=preprocess_input)\n","\n","validation_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n","\n","training_generator = training_datagen.flow_from_directory('fruits-360/Training/', target_size = (100,100), batch_size = 32, class_mode = 'categorical')\n","validation_generator = validation_datagen.flow_from_directory('fruits-360/Validation/', target_size = (100,100), batch_size = 32, class_mode = 'categorical')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Found 37836 images belonging to 75 classes.\n","Found 12709 images belonging to 75 classes.\n"],"name":"stdout"}]},{"metadata":{"id":"kK5lgk496GId","colab_type":"text"},"cell_type":"markdown","source":["## Task 2: Transfer Learning\n","\n","Next you will employ Transfer Learning and finetune the pretrained vgg-16 model you used in Task1 to better fit the fine-tune dataset D2 for 75 classes of fruits (details available in the readme file of dataset folder). You will:\n","\n","- Change the number of nodes in the last FC layer according to the number of classes i.e. 5 \n","- Freeze everything except the FC layers and train it using the train split of D2 (using appropriate hyperparameters), validating the network for validation split of data.\n","- Train (Finetune) the dataset with training split and validate it using validation split\n","- Plot loss/accuracy vs epochs curves for your simulation\n","\n","*You can use scikit-learn's `metrics.confusion_matrix` function. Consult the relevant documentation.* \n"]},{"metadata":{"id":"U4pntsfW6GIe","colab_type":"code","outputId":"21ac4277-fc41-4247-933e-ad0c747f2700","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"ok","timestamp":1555956423187,"user_tz":-300,"elapsed":1807,"user":{"displayName":"Umar Salman","photoUrl":"","userId":"02559536710924243729"}}},"cell_type":"code","source":["    \n","from keras import models\n","from keras import layers\n","from keras import optimizers\n","\n","vgg_imagenet = vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(100,100,3))\n","for lyr in vgg_imagenet.layers:\n","  lyr.training = False\n","\n"," \n","# Create the model\n","model = models.Sequential()\n"," \n","# Add the vgg convolutional base model\n","model.add(vgg_imagenet)\n"," \n","# Add new layers\n","model.add(layers.Flatten())\n","model.add(layers.Dense(1024, activation='relu'))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(75, activation='softmax'))\n"," \n","# Show a summary of the model. Check the number of trainable parameters\n","model.summary()\n","\n","# add new FC layers here\n","\n","# print summary and compile\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Model)                (None, 3, 3, 512)         14714688  \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 4608)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1024)              4719616   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 75)                76875     \n","=================================================================\n","Total params: 19,511,179\n","Trainable params: 19,511,179\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"5i6XpHB9PI8S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1458},"outputId":"30adde2c-1a18-4798-942d-a2ecdb240747","executionInfo":{"status":"error","timestamp":1555958364110,"user_tz":-300,"elapsed":1676992,"user":{"displayName":"Umar Salman","photoUrl":"","userId":"02559536710924243729"}}},"cell_type":"code","source":["# Compile the model\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-4),\n","              metrics=['acc'])\n","# Train the model\n","history = model.fit_generator(\n","      training_generator,\n","      steps_per_epoch=50 ,\n","      epochs=5,\n","      validation_data=validation_generator,\n","      validation_steps=validation_generator.samples/validation_generator.batch_size,\n","      verbose=1)\n"," \n","# Save the model\n","model.save('small_last4.h5')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","49/50 [============================>.] - ETA: 13s - loss: 4.5275 - acc: 0.0198"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-b9722ab9c693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       verbose=1)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    232\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             workers=0)\n\u001b[0m\u001b[1;32m    235\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"ekp3OdJTPjXC","colab_type":"code","colab":{}},"cell_type":"code","source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n"," \n","epochs = range(len(acc))\n"," \n","plt.plot(epochs, acc, 'b', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n"," \n","plt.figure()\n"," \n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n"," \n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"01nESzA-6GIj","colab_type":"code","colab":{}},"cell_type":"code","source":["#Loss accuracy curves using matplot lib"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kgYCjoKQ6GIm","colab_type":"text"},"cell_type":"markdown","source":["## Task 3: Network Evaluation\n","\n","Next you will test your finetuned model by plotting a confusion matrix between classes predicted. You will:\n","\n","- Load the saved finetuned network\n","- Test your model for images in validation folder of D2\n","- Construct a multiclass confusion matrix (for any 10 classes) for actual and predicted class of each image and visualize the confmatrix as a heatmap\n","\n","*You can use scikit-learn's `metrics.confusion_matrix` function. Consult the relevant documentation.* "]},{"metadata":{"id":"zSMyTeOK6GIn","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"ubHmE5IY6GIp","colab_type":"text"},"cell_type":"markdown","source":["## Task 4: Prediction\n","\n","Next you will test your finetuned model for test-multiple_fruits images and compare the result with the results of Task 1. You will:\n","\n","- Predict labels for images in test-multiple_fruits D1 folder using finetuned network\n","- Compare your result qualitativly and quatitatively (by visualizing some of the comparing images with their respective labels)\n","- Analyse and discuss the improvement of results (if found any)\n"]},{"metadata":{"id":"VTR78KjF6GIq","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","def makeIT():\n","  the_list =[]\n","  for filename in os.listdir(\"fruits-360/Training\"):\n","    the_list.append(filename)\n","  return the_list\n","\n","\n","the_list = makeIT()\n","\n","def printIT(the_list):\n","  for image in glob.glob('test-multiple_fruits/*.jpg'):\n","    img = load_img(image, target_size=(100, 100))\n","    arr = img_to_array(img)\n","    array = arr.reshape((1, 100, 100, 3)) \n","    pred = new_model.predict(array)\n","    pred = pred.reshape(-1)\n","    num_elem = len(image.split('/'))[1].split('_')\n","    print(\"-------------------\")\n","    print(\"Image: \",image)\n","    for i in range(num_elem):\n","      index = np.argmax(pred)\n","      print(the_list[index])\n","      pred[index]=-1\n","          \n","printIT(the_list)"],"execution_count":0,"outputs":[]}]}